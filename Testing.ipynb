{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c48158-5c5d-4068-935a-ef8c53ad387e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qle\\AppData\\Local\\miniconda3\\envs\\text\\Lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\qle\\AppData\\Local\\miniconda3\\envs\\text\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\qle\\AppData\\Local\\miniconda3\\envs\\text\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "### General Package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "### DL packages\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import albumentations # Data augmentation Package\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset\n",
    "pd.options.display.float_format ='{:.2f}'.format\n",
    "# %matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "# for colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa881cc-7c85-4ea7-ac7a-2ee3a942c4b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Plan: (44-roads 108-fields)\n",
    "- get the overall information of the image store in dataframe--> file.csv (Name(fiels-0), shape, information,direc, label ) for 2 file - train and test dataset\n",
    "- Image processing (reshape --> 128x128x3, normalize -->  (0,1)\n",
    "- Models (using pretrained model pytorch) - optimizers Adam - loss function: binary classification - (Basics one ) --> then improve by pretrained model --> See Unet Implementation file to see the procedure  Validation set- Hypermeter optimization\n",
    "- Build ML ops - (run-time -code) with PIL\n",
    "- Build docker image\n",
    "- Git preparation\n",
    "- Read paper Winter Road Surface Condition Recognition Using a Pre-trained Deep Convolutional Neural Network - using Pretrained Model VGG16\n",
    " \n",
    "WORKFLOW FOR ML(8 MAIN STEP) : (for Builing models)\n",
    "- Defind the problem and assembling a dataset\n",
    "- Choosing a measure of succes (AUC - Confusion Metrix)\n",
    "- Evaluation protocal (K-*crossvalidation,... )\n",
    "- Preparing the data (Data processing)\n",
    "- Develop the base model (for 1st working)\n",
    "- Sacling up (make model get overfit )\n",
    "- Regularization (overcome the overfit)\n",
    "- HyperParameters Tuning\n",
    "\n",
    "Can follow the mainstream in Chapter 5: Deep Learning (For Develop the base model)\n",
    "- Data splitting using os.path \n",
    "- Data preprocessing (tensor, Normailize, batch)\n",
    "- Data spliting (Training, Validation, Testing)\n",
    "- First Basic model: (using fit_generator)\n",
    "  + (4-Conv+MaxPooling --> to reudce shape shape from (150,150,3)->(7,7,128) before 2xFCL() --> 1 )\n",
    "  + Loss (binary_crossentropy) - optim(RMSprop) - metrics =['ACC']\n",
    "- See Results (develop until overfitting problem)\n",
    "- Deal with (Data augmentation - Dropout - Change the model architect -> using pretrained) -->See \n",
    "- Smoothing for better visualization (if curve is too noise)\n",
    "- Perform in test dataset\n",
    "\n",
    "Problem:\n",
    "\n",
    "1- How to choose proper Image Size for training (Done by following the paper)\n",
    "\n",
    "2- Data preprocessing- Vensorize - Normalize - Batch Generator ()\n",
    "\n",
    "3- Take a quick look (progrmaing Pytorch for Deep Learning) - to see the procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd86500d-e9df-4af1-8eb0-bb66ae5124c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters etc\n",
    "LEARNING_RATE = 1e-4\n",
    "# local \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 50\n",
    "NUM_WORKERS = 2 # unknown\n",
    "IMAGE_HEIGHT = 128 #150 # 1280 originally\n",
    "IMAGE_WIDTH = 128 # 1918 originally\n",
    "PIN_MEMORY = True # unknown \n",
    "LOAD_MODEL = False \n",
    "# Directrory \n",
    "TRIN_IMG_DIR = \"data/train_images/\"\n",
    "TRAIN_MASK_DIR = \"data/train_masks/\"\n",
    "VAL_IMG_DIR = \"data/val_imgaes/\"\n",
    "VAL_MASK_DIR = \"data/val_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d59dcfb-44a1-4b72-a1c3-d33846a9bb74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For Local \n",
    "data_path = 'dataset/'  \n",
    "savePath = 'SaveFile/'\n",
    "# For Colab\n",
    "#data_path = '/content/drive/MyDrive/Example/Trimble_dataset/dataset'\n",
    "#savePath = '/content/drive/MyDrive/Trimble/SaveFile/'\n",
    "\n",
    "classList = ['fields','roads']\n",
    "test = ['test_images']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a204a12-e199-4d12-a8d1-f45496e01cf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b59cee80-0498-4d69-8019-a35d46a263fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Data Generator\n",
    "def data_extractor(data_path,Class):\n",
    "    \"\"\"\n",
    "    fields = data_extractor(data_path,classList[0])\n",
    "    \"\"\"\n",
    "    all_names = os.listdir(data_path + Class + '/')\n",
    "    image_names = [el for el in all_names ]    \n",
    "    file = []\n",
    "    for img in image_names:        \n",
    "        file.append(data_path + Class+ '/' + img)\n",
    "    return file\n",
    "def Extract_Img_Info(Class):\n",
    "    '''\n",
    "    nameList, imgshapeList, imList = Extract_Img_Info(fields)\n",
    "    '''\n",
    "    imList=[]\n",
    "    imgshapeList=[]\n",
    "    nameList=[]\n",
    "    for img_path in Class:\n",
    "        img = cv2.imread(img_path)\n",
    "        imList.append(img)\n",
    "        imgshapeList.append(img.shape)\n",
    "        nameList.append(img_path.split('/')[1] + '-' +img_path.split('/')[2].split('.')[0])\n",
    "    return nameList , imgshapeList , imList\n",
    "def LabelExtraction(name,numLabel=True):\n",
    "    '''\n",
    "    Apply in the DataFrame\n",
    "    '''\n",
    "    label = name.split('-')[0]\n",
    "    if numLabel:\n",
    "        match label:\n",
    "            case 'fields':\n",
    "                 label = 0\n",
    "            case 'roads':\n",
    "                 label = 1\n",
    "            case _:\n",
    "                raise Exception(\"Not having in the class list\")\n",
    "    return label\n",
    "\n",
    "def DataGenerator(data_path, classList, savePath,save=True):\n",
    "    '''\n",
    "    Generate the .csv file for the dataset\n",
    "    df = DataGenerator(data_path, classList, savePath)\n",
    "    df_raw = pd.read_pickle(savePath+'dataset_raw.pkl')\n",
    "    '''\n",
    "    df = pd.DataFrame()\n",
    "    for category in classList:\n",
    "        category_data = data_extractor(data_path,category)\n",
    "        nameList, imgshapeList, imList = Extract_Img_Info(category_data)\n",
    "        data =[]\n",
    "        for i in range(len(nameList)):\n",
    "            data.append([nameList[i],imgshapeList[i],imList[i],category_data[i]])\n",
    "        df_cat = pd.DataFrame(data, columns=['ID','Img_Shape','Img_Info','Img_Dir'])\n",
    "        df = pd.concat([df, df_cat])    \n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop('index',axis=1,inplace=True)\n",
    "    df['label'] = df['ID'].apply(LabelExtraction)\n",
    "    if save:\n",
    "        if not os.path.exists(savePath):\n",
    "            os.makedirs(savePath)\n",
    "        df.to_pickle(savePath+ 'dataset_raw.pkl') # For Reuse \n",
    "        df.to_csv(savePath+'dataset_raw.csv') # For Read        \n",
    "    return df \n",
    "\n",
    "### Data Preprocessing\n",
    "def Normalize(x):\n",
    "    return x/255.0\n",
    "def Resize(x):\n",
    "    dim = (128,128)\n",
    "    return cv2.resize(x, dim, interpolation = cv2.INTER_AREA)\n",
    "def ImgShape(x):\n",
    "    return x.shape\n",
    "def LabelExtraction(name,numLabel=True):\n",
    "    '''\n",
    "    Apply in the DataFrame\n",
    "    '''\n",
    "    label = name.split('-')[0]\n",
    "    if numLabel:\n",
    "        match label:\n",
    "            case 'fields':\n",
    "                 label = 0\n",
    "            case 'roads':\n",
    "                 label = 1\n",
    "            case _:\n",
    "                raise Exception(\"Not having in the class list\")\n",
    "    return label\n",
    "def DataPreprocessing(df_raw,save=True):\n",
    "    '''\n",
    "    df = DataPreprocessing(df_raw)\n",
    "    df = pd.read_pickle(savePath+'dataset_raw.pkl')\n",
    "    '''\n",
    "    # reshape \n",
    "    df_raw['Img_Info_R'] = df_raw['Img_Info'].apply(Resize)    \n",
    "    # Normalize \n",
    "    df_raw['Img_Info_R_N'] = df_raw['Img_Info_R'].apply(Normalize)\n",
    "    # ImageShape \n",
    "    df_raw['Img_Shape_Resize'] = df_raw['Img_Info_R_N'].apply(ImgShape)\n",
    "    \n",
    "    # Rearrange the dataset    \n",
    "    dropList = ['Img_Info','Img_Info_R','Img_Shape']\n",
    "    df = df_raw.drop(dropList,axis=1)\n",
    "    #df['label'] = df['ID'].apply(LabelExtraction)\n",
    "    if save:\n",
    "        if not os.path.exists(savePath):\n",
    "            os.makedirs(savePath)\n",
    "        df.to_pickle(savePath+ 'dataset.pkl') # For Reuse \n",
    "        df.to_csv(savePath+'dataset.csv') # For Read \n",
    "    return df \n",
    "\n",
    "### Showing Image\n",
    "def showImg(image):\n",
    "    '''\n",
    "    # Normal just use plt.imshow() - this with high resolution\n",
    "    before process - showImg(df_raw['Img_Info'][0])\n",
    "    after process - showImg(df['Img_Info_R_N'][0])\n",
    "    '''\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "### Check information in loader\n",
    "# for X_batch in train_loader:\n",
    "#     print(X_batch)\n",
    "#     break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "544f90ac-747a-4c61-b848-c7612f0058d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FieldRoadDataset(Dataset):    \n",
    "    '''\n",
    "    trainData: dataFrame\n",
    "    return:\n",
    "    trainDs = FieldRoadDataset(trainData,transform=train_transform)\n",
    "    valDs = FieldRoadDataset(valData,transform=val_transform)\n",
    "    '''\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img_info = self.data['Img_Info']\n",
    "        labels = self.data['label']\n",
    "        if self.transform(image):\n",
    "            image = self.transform(img_info)\n",
    "        return [image, labels]\n",
    "    \n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "def load_checkpoint(checkpoint, model):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "def check_accuracy(loader, model, device=\"cuada\"):\n",
    "    num_correct = 0 \n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).unsqueeze(1)\n",
    "            preds = torch.sigma(model(x))\n",
    "            preds = (preds>0.5).float()\n",
    "            num_correct += (preds == y )/sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score +=(2*(preds*y).sum())/(\n",
    "              (preds+y).sum()+ 1e-8\n",
    "            )\n",
    "    print(f'Got {num_correct}/{num_pixels} with acc {num_correct}/{num_pixels*100:.2f}')\n",
    "    print(f'Dice score: {dice_score/len(loader)}')\n",
    "    model.train()\n",
    "\n",
    "def save_predictions_as_imgs(loader, model, folder=\"saved_images/\", device=\"cuda\"):\n",
    "    '''\n",
    "    Visualization of picture which is segmented\n",
    "    '''\n",
    "    model.eval()\n",
    "    for idx, (x,y) in enumerate(laoder):\n",
    "        x = x.to(device=device)\n",
    "    with torch.no_grad():\n",
    "        preds = torch.sigmoid(model(x))\n",
    "        preds = (preds > 0.5).float()\n",
    "\n",
    "    torchvision.utils.save_image(preds, f'{folder}/pred_{idx}.png')\n",
    "    torchvision.utils.save_image(y.unsqueeze(1),f'{folder}')\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37775996-6104-4a74-b25d-3bde2dd37a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.nn.parallel.data_parallel import device\n",
    "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
    "    '''\n",
    "    Training process\n",
    "    \n",
    "    '''\n",
    "    loop = tqdm(loader)\n",
    "\n",
    "    # Training 1 epoch\n",
    "    for batch_idx, (data,targets) in enumerate(loop):\n",
    "        data = data.to(device = DEVICE)\n",
    "        # Convert to the float and unsqueeze is reshape the target --> just 1 dimension\n",
    "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "        # forwad\n",
    "        with torch.cuda.amp.autocast():\n",
    "            '''\n",
    "            Instances of torch.autocast enable autocasting for chosen regions. \n",
    "            Autocasting automatically chooses the precision for GPU operations \n",
    "            to improve performance while maintaining accuracy.\n",
    "            '''\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions,targets)\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "    # Backward passes under autocast are not recommended.\n",
    "    # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "    scaler.scale(loss).backward()\n",
    "    # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "    # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "    # otherwise, optimizer.step() is skipped.\n",
    "    scaler.step(optimizer)\n",
    "    # Updates the scale for next iteration.\n",
    "    scaler.update()\n",
    "\n",
    "    # update tqdm loop\n",
    "    loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# def main():\n",
    "#     '''\n",
    "#     train()\n",
    "#     '''\n",
    "#     model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
    "#     # need change\n",
    "#     loss_fn = nn.BCEWithLogitsLoss() # B/c the output of the model doesn't have sigmoi activation\n",
    "#     # If having - use Binary Cross Entropy for 1 class segmentation\n",
    "#     # - multi class segmentation out_channel=3 +Lf: cross-Entropy loss\n",
    "    \n",
    "#     # Optimizer initialization\n",
    "#     optimizer = optim.Adam(model.parametrers(), lr=LEARNING_RATE)\n",
    "    \n",
    "#     if LOAD_MODEL:\n",
    "#     load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n",
    "\n",
    "#     scaler = torch.cuda.amp.GradScaler()\n",
    "#     for epoch in range(NUM_EPOCHS):\n",
    "#         train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
    "\n",
    "#         #save model\n",
    "#         checkpoint = {\n",
    "#             \"state_dict\": model.state_dict(),\n",
    "#             \"optimizer\": optimizer.state_dict(),\n",
    "#         }\n",
    "#         save_checkpoint(checkpoint)\n",
    "\n",
    "#         # CHeck accuracy\n",
    "#         check_accuracy(val_loader, model, device=DEVICE)\n",
    "#         # Print some examples for illustrations\n",
    "#         save_predictions_as_imgs(\n",
    "#                                 val_loader, \n",
    "#                                 model, \n",
    "#                                 folder=\"saved_images/\", \n",
    "#                                 device=DEVICE,\n",
    "#                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0e137d6-0b78-4fec-a01c-d6b36a73b946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MODEL(nn.Module):\n",
    "    '''\n",
    "    The simple Model Th input (16,128,128,3)\n",
    "    2x Conv+BN+MP(feature extraction) + 2xFCL(classifier)\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize the parameter\n",
    "        Inhereritence the nn.Module + modify the parameters which suitable for our model\n",
    "        model = Net()\n",
    "        '''\n",
    "        super(MODEL, self).__init__()\n",
    "        # 1st CNN layer - input chanel tensor \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 2nd CNN layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)        \n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        # 1st fully layer\n",
    "        self.fc1 = nn.Linear(in_features=64 * 32 * 32, out_features=64)\n",
    "        # 2nd fully layer\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=2)\n",
    "        # activation layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x): # Input shape : [16,3,128,128] # 64 : batch size\n",
    "        # 1st layer -> relu []\n",
    "        x = self.relu(self.conv1(x)) # [16,32,128,128]\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool(x) # [16,32,64,64]\n",
    "        # 2nd layer -> relu\n",
    "        x = self.relu(self.conv2(x)) # [16,64,64,64]\n",
    "        x = self.bn2(x)        \n",
    "        x = self.pool(x) # [16,64,32,32]       \n",
    "            \n",
    "        # flattern layer\n",
    "        x = x.view(-1, 64*32*32)# [16,128]\n",
    "        # 1st fully layer + relu\n",
    "        x = self.relu(self.fc1(x)) # [64,64]\n",
    "        # 2nd fully connected model\n",
    "        x = self.fc2(x) # [64,2] # 2 - number of labels\n",
    "        # Check whether the last layer is neccesary\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e9cd2f6-f1f8-4589-8a77-1b906b15e0b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def TrainValSplitting(df,TrainRatio=0.75,fnac=1):\n",
    "    '''\n",
    "    trainData, valData = TrainValSplitting(df,TrainRatio)\n",
    "    '''    \n",
    "    df = df.sample(frac = 1)\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop('index',axis=1,inplace=True)\n",
    "    spliitingInd = math.ceil(len(df)*TrainRatio)\n",
    "    train, val = df[:spliitingInd], df[spliitingInd:]\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71132908-d92b-472d-ad0e-d68ba4cf00d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72e4597f-0e87-40a3-936b-e126230c999f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TrainRatio = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "507cc736-d7bd-4c6e-9a26-2ec14b3942a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract dataset and text set\n",
    "df = DataGenerator(data_path, classList, savePath)\n",
    "#text_raw = DataGenerator(data_path, classList, savePath)\n",
    "\n",
    "# Train and Val splitting 115(1-84) - 38\n",
    "trainData, valData = TrainValSplitting(df,TrainRatio) \n",
    "\n",
    "# Data preprocessing - uncheck\n",
    "train_transform = albumentations.Compose(\n",
    "      [\n",
    "          albumentations.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "          ### Dama Augmentation\n",
    "          #A.Rotate(limit=35, p=1.0),\n",
    "          #A.HorizontalFlip(p=0.5),\n",
    "          #A.VerticalFlip(p=0.1),\n",
    "          # TO Tensor doesn't divide by 255 like Pytorch\n",
    "          # it's done inside Normalize function\n",
    "          albumentations.Normalize(\n",
    "              mean=[0.0, 0.0, 0.0],\n",
    "              std = [1.0, 1.0, 1.0],\n",
    "              max_pixel_value = 255.0,\n",
    "          ),\n",
    "          ToTensorV2()\n",
    "      ]\n",
    "  )\n",
    "val_transform = albumentations.Compose(\n",
    "      [\n",
    "          albumentations.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "          # TO Tensor doesn't divide by 255 like Pytorch\n",
    "          # it's done inside Normalize function\n",
    "          albumentations.Normalize(\n",
    "              mean=[0.0, 0.0, 0.0],\n",
    "              std = [1.0, 1.0, 1.0],\n",
    "              max_pixel_value = 255.0,\n",
    "          ),\n",
    "          ToTensorV2()\n",
    "      ]\n",
    "  )\n",
    "\n",
    "trainDs = FieldRoadDataset(trainData,transform=train_transform)\n",
    "valDs = FieldRoadDataset(valData,transform=val_transform)\n",
    "\n",
    "# loader with processing\n",
    "trainLoader = DataLoader(trainDs,batch_size = BATCH_SIZE,num_workers = NUM_WORKERS,pin_memory = PIN_MEMORY,shuffle=False) \n",
    "valLoader = DataLoader(trainDs,batch_size = BATCH_SIZE,num_workers = NUM_WORKERS,pin_memory = PIN_MEMORY,shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ddcff-c0d4-4830-a777-a6096176240b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qle\\AppData\\Local\\miniconda3\\envs\\text\\Lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Model Initilization\n",
    "model = MODEL().to(DEVICE)\n",
    "# need change\n",
    "loss_fn = nn.BCEWithLogitsLoss() # B/c the output of the model doesn't have sigmoi activation\n",
    "# If having - use Binary Cross Entropy for 1 class segmentation\n",
    "# - multi class segmentation out_channel=3 +Lf: cross-Entropy loss\n",
    "\n",
    "# Optimizer initialization\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_fn(trainLoader, model, optimizer, loss_fn, scaler)\n",
    "\n",
    "    #save model\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    save_checkpoint(checkpoint)\n",
    "\n",
    "    # CHeck accuracy\n",
    "    check_accuracy(valLoader, model, device=DEVICE)\n",
    "    # # Print some examples for illustrations\n",
    "    # save_predictions_as_imgs(\n",
    "    #                         val_loader, \n",
    "    #                         model, \n",
    "    #                         folder=\"saved_images/\", \n",
    "    #                         device=DEVICE,\n",
    "    #                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5135788f-16bf-471b-bedd-ad59bc2de474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
    "    '''\n",
    "    Training process    \n",
    "    '''\n",
    "    loop = tqdm(loader)\n",
    "\n",
    "    # Training 1 epoch\n",
    "    for batch_idx, (data,targets) in enumerate(loop):\n",
    "        data = data.to(device = DEVICE)\n",
    "        # Convert to the float and unsqueeze is reshape the target --> just 1 dimension\n",
    "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "        # forwad\n",
    "        with torch.cuda.amp.autocast():\n",
    "        '''\n",
    "        Instances of torch.autocast enable autocasting for chosen regions. \n",
    "        Autocasting automatically chooses the precision for GPU operations \n",
    "        to improve performance while maintaining accuracy.\n",
    "        '''\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions,targets)\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "    # Backward passes under autocast are not recommended.\n",
    "    # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "    scaler.scale(loss).backward()\n",
    "    # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "    # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "    # otherwise, optimizer.step() is skipped.\n",
    "    scaler.step(optimizer)\n",
    "    # Updates the scale for next iteration.\n",
    "    scaler.update()\n",
    "\n",
    "    # update tqdm loop\n",
    "    loop.set_postfix(loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246478f8-b0f4-49a1-bf5f-9ad3e5440d12",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2334e7cd-3303-4781-856e-9d3a160f4c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1 : Prepare the data\n",
    "# Extract the information\n",
    "X = df['Img_Info_R_N'] # 153 samples with (128,128,3)\n",
    "#y = df['label'] # 153\n",
    "# Convert into Torch Tensor\n",
    "train_data = torch.tensor(X, dtype= torch.float32) # torch.size([153,128,128,3])\n",
    "#text_data = torch.tensor(y, dtype= torch.float32).reshape(-1,1) # reshape --> build the torch with Ex -(153,1)\n",
    "# Split them into batch \n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)# Change it into  torch.size([16,128,128,3])\n",
    "val_loader = DataLoader\n",
    "#test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5315577e-3a23-4440-8c67-9b3652a52a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel.data_parallel import device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64130718-99c3-489b-b0ee-eec12903f355",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qle\\AppData\\Local\\miniconda3\\envs\\text\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\qle\\AppData\\Local\\miniconda3\\envs\\text\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\qle/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 528M/528M [00:20<00:00, 26.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "# dir(models) - show all model - AlexNet --> Function while alexnet --> pretrained model\n",
    "# https://www.kaggle.com/code/pvlima/use-pretrained-pytorch-models/notebook - using this reference for building the top of model\n",
    "vgg16 = models.vgg16(pretrained=True) # call the vgg\n",
    "#print(vgg16)\n",
    "\n",
    "# freeze all model parameters\n",
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# new final layer with 16 classes\n",
    "num_ftrs = vgg16.fc.in_features\n",
    "resnet.fc = torch.nn.Linear(num_ftrs, 16)\n",
    "if use_gpu:\n",
    "    resnet = resnet.cuda()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(resnet.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "dloaders = {'train':train_dl, 'valid':valid_dl}\n",
    "\n",
    "start_time = time.time()\n",
    "model = train_model(dloaders, resnet, criterion, optimizer, exp_lr_scheduler, num_epochs=2)\n",
    "print('Training time: {:10f} minutes'.format((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b343bebb-a0be-4913-a1d1-ba3edaf9d706",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Prepare the data\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "# Step 2: Define the model\n",
    "class Net(nn.Module):\n",
    "    # Always build Model based on OOP\n",
    "    # This is simple model\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize the parameter\n",
    "        Inhereritence the nn.Module + modify the parameters which suitable for our model\n",
    "        '''\n",
    "        super(Net, self).__init__()\n",
    "        # 1st CNN layer -input chanel tensor\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        # BatchNormalization 1\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        # MaxPool2D layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 2nd CNN layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        # BatchNormalization 1\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        # 1st fully layer\n",
    "        self.fc1 = nn.Linear(in_features=32 * 8 * 8, out_features=64)\n",
    "        # 2nd fully layer\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=10)\n",
    "        # activation layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x): # Input shape : [64,3,32,32] # 64 : batch size\n",
    "        # 1st layer -> relu []\n",
    "        x = self.relu(self.conv1(x)) # [64,16,32,32]\n",
    "        # max pooling layer - 2D -> Reduce half\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool(x) # [64,16,16,16]\n",
    "        # 2nd layer -> relu\n",
    "        x = self.relu(self.conv2(x)) # [64,32,16,16]\n",
    "        x = self.bn2(x)\n",
    "        # max pooling layer\n",
    "        x = self.pool(x) # [64,32,8,8]\n",
    "        # flattern layer\n",
    "        x = x.view(-1, 32 * 8 * 8)# [64,2048]\n",
    "        # 1st fully layer + relu\n",
    "        x = self.relu(self.fc1(x)) # [64,64]\n",
    "        # 2nd fully connected model\n",
    "        x = self.fc2(x) # [64,10] # 10 - number of labels\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()\n",
    "# Step 3: Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Step 4: Define the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# Step 5: Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    # tqdm package for the interation looks smarter\n",
    "    for i, data in tqdm(enumerate(train_loader, 0)):\n",
    "        # Extract the data and label data from traning dataset\n",
    "        inputs, labels = data\n",
    "        # Set optimizer tensor to 0\n",
    "        optimizer.zero_grad()\n",
    "        # Set up the model with inputs\n",
    "        outputs = model(inputs)\n",
    "        # Runing the loss function\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        # Launcing the optimizer method --> Update for the new weight\n",
    "        optimizer.step()\n",
    "        # Cummulate the loss value\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "model.eval() # turn the mode\n",
    "total_correct = 0\n",
    "a=0\n",
    "with torch.no_grad():\n",
    "    '''\n",
    "    A loop where every tensor inside the loop will have requires_grad set to False\n",
    "    --> It means any tensor with gradient currently attached with\n",
    "    the current computational graph is now detached from the current graph\n",
    "    '''\n",
    "    for images, labels in test_loader:\n",
    "        # Prediction\n",
    "        outputs = model(images)\n",
    "        # Get the predicted value with highest probability\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        # if predicted == labels --> 1 then sum all\n",
    "        # the same with or without item\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = total_correct / len(test_data)\n",
    "print(f'Test accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7209aeba-29a2-4dc6-acc7-a2173f1d8120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
